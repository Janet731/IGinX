name: Run TPC-H Queries with Spark

on:
    push:
        branches: [main]
    pull_request:
        branches: [main]

jobs:
    setup-and-run-tpch:
        runs-on: ubuntu-latest

        services:
            hadoop-namenode:
                image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8
                ports:
                    - 9870:9870
                    - 9000:9000
                options: --name hadoop-namenode -e CLUSTER_NAME=test -e HDFS_CONF_dfs_replication=1
            hadoop-datanode:
                image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8
                options: --name hadoop-datanode --link hadoop-namenode:namenode
            spark:
                image: bde2020/spark-master:2.4.0-hadoop2.7
                ports:
                    - 8080:8080
                options: --name spark --link hadoop-namenode:namenode

        steps:
            - name: Checkout repository
              uses: actions/checkout@v2

            - name: Install dependencies
              run: |
                  sudo apt-get update
                  sudo apt-get install -y openjdk-8-jdk maven wget

            - name: Install Hadoop
              run: |
                  wget https://archive.apache.org/dist/hadoop/core/hadoop-2.7.4/hadoop-2.7.4.tar.gz
                  tar -xzf hadoop-2.7.4.tar.gz
                  sudo mv hadoop-2.7.4 /usr/local/hadoop

            - name: Configure Hadoop
              run: |
                  cat <<EOF > /usr/local/hadoop/etc/hadoop/core-site.xml
                  <configuration>
                      <property>
                          <name>fs.defaultFS</name>
                          <value>hdfs://localhost:9000</value>
                      </property>
                  </configuration>
                  EOF

            - name: Set up Hadoop environment variables
              run: |
                  echo "HADOOP_HOME=/usr/local/hadoop" >> $GITHUB_ENV
                  echo "HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop" >> $GITHUB_ENV
                  echo "PATH=$PATH:/usr/local/hadoop/bin" >> $GITHUB_ENV
                  echo "HADOOP_INSTALL=/usr/local/hadoop" >> $GITHUB_ENV
                  echo "HADOOP_MAPRED_HOME=/usr/local/hadoop" >> $GITHUB_ENV
                  echo "HADOOP_COMMON_HOME=/usr/local/hadoop" >> $GITHUB_ENV
                  echo "HADOOP_HDFS_HOME=/usr/local/hadoop" >> $GITHUB_ENV
                  echo "YARN_HOME=/usr/local/hadoop" >> $GITHUB_ENV

            - name: Check HDFS command
              run: |
                  source $GITHUB_ENV
                  hdfs version

            - name: Install Spark
              run: |
                  wget http://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz
                  tar xvf spark-2.4.0-bin-hadoop2.7.tgz
                  echo "SPARK_HOME=$(pwd)/spark-2.4.0-bin-hadoop2.7" >> $GITHUB_ENV
                  echo "PATH=$SPARK_HOME/bin:$PATH" >> $GITHUB_ENV

            - name: Generate TPC-H data
              run: |
                  pip install requests tqdm
                  python3 thu_cloud_download.py -l https://cloud.tsinghua.edu.cn/d/740c158819bc4759a36e/ -s  "."
                  cd tpchdata
                  destination_folder="../tpch_data"
                  mkdir -p "$destination_folder"
                  mv *.tbl "$destination_folder/"
                  cd "$destination_folder"
                  ls -a
                  pwd
                  echo "文件移动完成"

            - name: Wait for Hadoop to start
              run: |
                  source $GITHUB_ENV
                  echo "Waiting for Hadoop to start..."
                  for i in {1..20}; do
                    hdfs dfsadmin -safemode get | grep -q 'Safe mode is OFF'
                    if [ $? -eq 0 ]; then
                      echo "Hadoop is up and running."
                      break
                    fi
                    echo "Hadoop is not ready yet. Waiting..."
                    sleep 30
                  done

            - name: Leave HDFS safemode
              run: |
                  source $GITHUB_ENV
                  hdfs dfsadmin -safemode leave

            - name: Upload TPC-H data to HDFS
              run: |
                  source $GITHUB_ENV
                  hdfs dfs -mkdir -p /tpchData
                  hdfs dfs -put tpch_data/*.tbl /tpchData/
                  hdfs dfs -ls /tpchData

            - name: Compile TPC-H Query Java Program
              run: |
                  mvn clean package

            - name: Run TPC-H Query 1
              run: |
                  source $GITHUB_ENV
                  $SPARK_HOME/bin/spark-submit --master spark://spark:7077 --class com.example.tpch.TpchQueryRunner target/tpch-query-1.0-SNAPSHOT-jar-with-dependencies.jar
